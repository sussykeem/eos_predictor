# Base config for MLP (regression)

network:
  type: MLP
  input_dim: 64
  output_dim: 2  # for a and b constants

layers:
  - type: FC
    units: 256
    activation: ReLU
    dropout: 0.2
  - type: FC
    units: 128
    activation: ReLU
    dropout: 0.2
  - type: FC
    units: 64
    activation: ReLU
    dropout: 0.1
  - type: FC
    units: 2
    activation: None  # linear output for regression

optimizer:
  type: Adam
  learning_rate: 0.001
  decay: 0.0001

loss_function: MSELoss

training:
  batch_size: 64
  epochs: 20

data:
  train_data: path/to/train_data
  test_data: path/to/test_data
  validation_data: path/to/validation_data

seed: 42
gpu: true
